# âš¡ Foundry SpeedTest

> **Matrix-themed CLI benchmark suite for Azure AI Foundry models**
>
> Compare **Completions API** vs **Responses API** head-to-head with real latency metrics, throughput analysis, and cache behaviour testing â€” all from your terminal.

```
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
 â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
 â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—  â•šâ–ˆâ–ˆâ•”â•
 â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
 â•šâ•â•     â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•   â•šâ•â•
  â–‘â–’â–“ S P E E D T E S T  Â·  M i c r o s o f t  F o u n d r y â–“â–’â–‘
```

---

## ğŸ“Š What It Measures

| Metric | Description |
|--------|-------------|
| **TTFT** | Time to First Token â€” how fast the stream starts (streaming mode) |
| **Total Time** | Wall-clock time for the full response |
| **Tokens/sec (TPS)** | Output throughput â€” tokens generated per second |
| **Input / Output Tokens** | Token counts reported by the API |
| **Cache Hit / Miss** | Detects prompt caching via `cached_tokens` in usage |
| **Concurrent Throughput** | Parallel request performance under load |
| **P50 / P90 / P99** | Percentile latencies across multiple runs |
| **Error Rate** | Failed requests as a percentage of total |

Tests run against **both** the **Completions API** (`chat.completions.create`) and the **Responses API** (`responses.create`) in streaming and non-streaming modes.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLI Layer (Fire + Rich)                    â”‚
â”‚   bench  Â·  quick  Â·  compare  Â·  list_prompts               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Benchmark Engine                           â”‚
â”‚  benchmark_prompt Â· benchmark_cache Â· benchmark_concurrency   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Adapters                              â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â”‚ Completions  â”‚    â”‚  Responses   â”‚                 â”‚
â”‚         â”‚  (stream +   â”‚    â”‚  (stream +   â”‚                 â”‚
â”‚         â”‚   sync)      â”‚    â”‚   sync)      â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Azure AI Foundry Endpoint                        â”‚
â”‚         (OpenAI-compatible  Â·  Entra ID Auth)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ SingleRunMetricsâ”‚â”€â”€â”€â”€â–¶â”‚AggregateMetricsâ”‚
        â”‚ (per-call data)â”‚     â”‚ (P50/P90/P99)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚         Reporter            â”‚
                         â”‚  JSON Â· CSV Â· Raw CSV Log   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> Full Mermaid diagram: [`docs/architecture.mmd`](docs/architecture.mmd)

---

## ğŸ“ Project Structure

```
Foundry-SpeedTest/
â”œâ”€â”€ foundry_speedtest/          # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py             # python -m entry point
â”‚   â”œâ”€â”€ cli.py                  # Fire CLI + Rich Matrix UI
â”‚   â”œâ”€â”€ benchmarks.py           # Test orchestrator
â”‚   â”œâ”€â”€ adapters.py             # Completions & Responses API wrappers
â”‚   â”œâ”€â”€ metrics.py              # SingleRunMetrics + AggregateMetrics
â”‚   â”œâ”€â”€ config.py               # Prompts, BenchmarkConfig
â”‚   â””â”€â”€ reporter.py             # JSON/CSV export
â”œâ”€â”€ API/                        # Sample standalone scripts
â”‚   â”œâ”€â”€ completions_connection.py
â”‚   â””â”€â”€ responses_connection.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.mmd        # Mermaid architecture diagram
â”œâ”€â”€ .env.example                # Template for env vars
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Azure AI Foundry** resource with a deployed model
- **Azure CLI** logged in (`az login`) â€” used for Entra ID authentication

### 1. Clone & Setup

```bash
git clone https://github.com/<your-username>/Foundry-SpeedTest.git
cd Foundry-SpeedTest

# Create virtual environment
python -m venv .venv

# Activate it
# Windows PowerShell:
.venv\Scripts\Activate.ps1
# macOS/Linux:
source .venv/bin/activate
```

### 2. Install

```bash
pip install -e .
```

### 3. Configure

```bash
# Copy the example env file
cp .env.example .env

# Edit .env with your Foundry endpoint
# AZURE_FOUNDRY_ENDPOINT=https://<your-resource>.openai.azure.com/openai/v1
```

### 4. Authenticate

```bash
# Make sure you're logged in to Azure
az login
```

### 5. Run

```bash
# Quick connectivity check
python -m foundry_speedtest quick gpt-4.1-nano

# Full benchmark suite
python -m foundry_speedtest bench gpt-4.1-nano

# Compare two models head-to-head
python -m foundry_speedtest compare gpt-4.1-nano gpt-4.1-mini
```

---

## ğŸ”¥ CLI Commands

### `bench` â€” Full Benchmark Suite

```bash
python -m foundry_speedtest bench <model> [OPTIONS]
```

| Option | Default | Description |
|--------|---------|-------------|
| `--iterations` | `3` | Measured runs per prompt/API combo |
| `--warmup` | `1` | Warmup runs (discarded) |
| `--max_tokens` | `512` | Max output tokens per request |
| `--temperature` | `0.7` | Sampling temperature |
| `--concurrency` | `5` | Parallel requests for throughput test |
| `--cache_rounds` | `5` | Identical calls for cache detection |
| `--apis` | `both` | `both`, `completions`, or `responses` |
| `--prompts` | `all` | `all` or comma-separated: `short,medium,long,code,reasoning` |
| `--timeout` | `120` | Request timeout (seconds) |
| `--output` | `all` | Export: `all`, `json`, `csv`, or `none` |

**Examples:**

```bash
# Test only Completions API with 5 iterations
python -m foundry_speedtest bench gpt-5.2 --apis completions --iterations 5

# Short + code prompts only, no file export
python -m foundry_speedtest bench gpt-4.1-nano --prompts short,code --output none

# High concurrency stress test
python -m foundry_speedtest bench gpt-4.1-nano --concurrency 20 --cache_rounds 10
```

### `quick` â€” Fast Connectivity Check

```bash
python -m foundry_speedtest quick <model>
```

Runs a single medium prompt across both APIs (streaming + sync) â€” good for verifying your setup works.

### `compare` â€” Model vs Model

```bash
python -m foundry_speedtest compare <model1> <model2>
```

Side-by-side comparison across short/medium/long prompts with a winner declared per metric.

### `list_prompts` â€” Show Prompt Catalogue

```bash
python -m foundry_speedtest list_prompts
```

---

## ğŸ“‚ Output & Reports

Results are saved to the `results/` directory:

| File | Contents |
|------|----------|
| `benchmark_<model>_<timestamp>.json` | Full structured results with nested stats |
| `benchmark_<model>_<timestamp>.csv` | Aggregate summary (one row per test) |
| `benchmark_<model>_<timestamp>_raw.csv` | Every individual API call logged |

---

## ğŸ§ª What Gets Tested

| Test | Streaming | Non-Streaming | Description |
|------|:---------:|:-------------:|-------------|
| **Short prompt** | âœ“ | âœ“ | Trivial question â†’ measures overhead |
| **Medium prompt** | âœ“ | âœ“ | Technical explanation â†’ balanced test |
| **Long prompt** | âœ“ | âœ“ | Tutorial generation â†’ heavy output |
| **Code generation** | âœ“ | âœ“ | Python function â†’ structured output |
| **Reasoning** | âœ“ | âœ“ | Multi-step logic â†’ reasoning latency |
| **Cache warm/cold** | âœ“ | â€” | Identical prompt repeated N times |
| **Concurrency** | â€” | âœ“ | Parallel requests for throughput |

---

## ğŸ” Authentication

This tool uses **Azure Entra ID** (via `DefaultAzureCredential`) â€” no API keys stored in code. It picks up credentials from:

1. `az login` session
2. Environment variables (`AZURE_CLIENT_ID`, etc.)
3. Managed Identity (when running in Azure)
4. VS Code Azure account

---

## License

MIT
