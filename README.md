# âš¡ Foundry SpeedTest

> **Matrix-themed CLI benchmark suite for Azure AI Foundry models**
>
> Compare **Completions API** vs **Responses API** head-to-head with real latency metrics, throughput analysis, and cache behaviour testing â€” all from your terminal.

```
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
 â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
 â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—  â•šâ–ˆâ–ˆâ•”â•
 â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
 â•šâ•â•     â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•   â•šâ•â•
  â–‘â–’â–“ S P E E D T E S T  Â·  M i c r o s o f t  F o u n d r y â–“â–’â–‘
```

---

## ğŸ“Š What It Measures

| Metric | Description |
|--------|-------------|
| **TTFT** | Time to First Token â€” how fast the stream starts (streaming mode) |
| **Total Time** | Wall-clock time for the full response |
| **Tokens/sec (TPS)** | Output throughput â€” tokens generated per second |
| **Input / Output Tokens** | Token counts reported by the API |
| **Cache Hit / Miss** | Detects prompt caching via `cached_tokens` in usage |
| **Concurrent Throughput** | Parallel request performance under load |
| **P50 / P90 / P99** | Percentile latencies across multiple runs |
| **Error Rate** | Failed requests as a percentage of total |

Tests run against **both** the **Completions API** (`chat.completions.create`) and the **Responses API** (`responses.create`) in streaming and non-streaming modes.

---

## ğŸ“– Metric Glossary

### Column Reference (Live Results & Run Tables)

| Column | Meaning |
|--------|---------|
| **#** | Sequential run number |
| **API** | Which API was used â€” `completions` (cyan) or `responses` (magenta) |
| **Prompt / Test** | The benchmark prompt set that was used (e.g. "Short (trivial)", "Cache test") |
| **Stream** | `âœ“` = streaming mode, `â€”` = non-streaming (sync) mode |
| **Mode** | `â‡£` = streaming, `â—` = sync (compact form used in the live panel) |
| **TTFT** | **Time to First Token** â€” time from request sent to first streamed chunk received. Only measured in streaming mode; shows `â€”` for sync requests |
| **Total** | **Total Time** â€” wall-clock duration from request start to final token received |
| **TPS** | **Tokens Per Second** â€” `output_tokens / total_time`. Measures generation throughput |
| **In Tok** | Number of input (prompt) tokens as reported by the API's `usage` object |
| **Out Tok / Out** | Number of output (completion) tokens generated |
| **Cached** | Number of prompt tokens served from the server-side prompt cache (`cached_tokens` from usage). `â€”` or `Â·` when zero |
| **Cache** | Same as Cached, compact form in the live panel |
| **Status** | `âœ“` (green) = request succeeded Â· `âœ—` (red) = request failed with an error |

### Aggregate Statistics

| Stat | Meaning |
|------|---------|
| **Runs** | Total number of measured runs (excludes warmup) |
| **Err%** | Percentage of runs that returned an error |
| **Mean** | Arithmetic average across all successful runs |
| **Median / P50** | Middle value â€” 50th percentile |
| **P90** | 90th percentile â€” value below which 90% of runs fall. Represents "worst realistic case" |
| **P99** | 99th percentile â€” tail latency |
| **StdDev** | Standard deviation â€” measures consistency. Low = stable, high = variable |
| **Avg In / Avg Out** | Average input and output token counts across runs |
| **Cache Hit%** | Percentage of runs where `cached_tokens > 0` |

### Cold Start Panel

| Indicator | Meaning |
|-----------|---------|
| **1st TTFT** | Time to first token on the very first streaming request. Often higher due to model loading or container cold start |
| **Avg TTFT (rest)** | Average TTFT of all subsequent streaming requests (after the first) |
| **Cold Penalty** | `1st TTFT âˆ’ Avg TTFT (rest)`. Positive = first call was slower. Shown in ms |
| **â–“â–“â–“ COLD** | Cold penalty > 100 ms â€” significant cold start detected |
| **â–“â–“â–‘ WARMING UP** | Cold penalty 30â€“100 ms â€” mild warm-up overhead |
| **â–“â–‘â–‘ HOT** | Cold penalty < 30 ms â€” endpoint was already warm |
| **Cache Hit Rate** | Fraction of successful runs where cached tokens were returned |
| **Cached Tokens** | Total cached tokens across all runs in the session |

### TTFT / TPS Colour Coding

| Colour | TTFT Meaning | TPS Meaning |
|--------|-------------|-------------|
| ğŸŸ¢ **Green** | < 200 ms â€” fast | > 80 tok/s â€” fast |
| ğŸŸ¡ **Yellow** | 200â€“500 ms â€” moderate | 30â€“80 tok/s â€” moderate |
| ğŸ”´ **Red** | > 500 ms â€” slow | < 30 tok/s â€” slow |

### Head-to-Head Comparison

When running with `--apis both` (default), a comparison panel shows each metric for Completions vs Responses with the **Winner** highlighted â€” lower is better for latency metrics, higher is better for throughput.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLI Layer (Fire + Rich)                    â”‚
â”‚   bench  Â·  quick  Â·  compare  Â·  list_prompts               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Benchmark Engine                           â”‚
â”‚  benchmark_prompt Â· benchmark_cache Â· benchmark_concurrency   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Adapters                              â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â”‚ Completions  â”‚    â”‚  Responses   â”‚                 â”‚
â”‚         â”‚  (stream +   â”‚    â”‚  (stream +   â”‚                 â”‚
â”‚         â”‚   sync)      â”‚    â”‚   sync)      â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Azure AI Foundry Endpoint                        â”‚
â”‚         (OpenAI-compatible  Â·  Entra ID Auth)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ SingleRunMetricsâ”‚â”€â”€â”€â”€â–¶â”‚AggregateMetricsâ”‚
        â”‚ (per-call data)â”‚     â”‚ (P50/P90/P99)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚         Reporter            â”‚
                         â”‚  JSON Â· CSV Â· Raw CSV Log   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> Full Mermaid diagram: [`docs/architecture.mmd`](docs/architecture.mmd)

---

## ğŸ“ Project Structure

```
Foundry-SpeedTest/
â”œâ”€â”€ foundry_speedtest/          # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py             # python -m entry point
â”‚   â”œâ”€â”€ cli.py                  # Fire CLI + Rich Matrix UI
â”‚   â”œâ”€â”€ benchmarks.py           # Test orchestrator
â”‚   â”œâ”€â”€ adapters.py             # Completions & Responses API wrappers
â”‚   â”œâ”€â”€ metrics.py              # SingleRunMetrics + AggregateMetrics
â”‚   â”œâ”€â”€ config.py               # Prompts, BenchmarkConfig
â”‚   â””â”€â”€ reporter.py             # JSON/CSV export
â”œâ”€â”€ API/                        # Sample standalone scripts
â”‚   â”œâ”€â”€ completions_connection.py
â”‚   â””â”€â”€ responses_connection.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.mmd        # Mermaid architecture diagram
â”œâ”€â”€ .env.example                # Template for env vars
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Azure AI Foundry** resource with a deployed model
- **Azure CLI** logged in (`az login`) â€” used for Entra ID authentication

### 1. Clone & Setup

```bash
git clone https://github.com/<your-username>/Foundry-SpeedTest.git
cd Foundry-SpeedTest

# Create virtual environment
python -m venv .venv

# Activate it
# Windows PowerShell:
.venv\Scripts\Activate.ps1
# macOS/Linux:
source .venv/bin/activate
```

### 2. Install

```bash
pip install -e .
```

### 3. Configure

```bash
# Copy the example env file
cp .env.example .env

# Edit .env with your Foundry endpoint
# AZURE_FOUNDRY_ENDPOINT=https://<your-resource>.openai.azure.com/openai/v1
```

### 4. Authenticate

```bash
# Make sure you're logged in to Azure
az login
```

### 5. Run

```bash
# Quick connectivity check
python -m foundry_speedtest quick gpt-4.1-nano

# Full benchmark suite
python -m foundry_speedtest bench gpt-4.1-nano

# Compare two models head-to-head
python -m foundry_speedtest compare gpt-4.1-nano gpt-4.1-mini
```

---

## ğŸ”¥ CLI Commands

### `bench` â€” Full Benchmark Suite

```bash
python -m foundry_speedtest bench <model> [OPTIONS]
```

| Option | Default | Description |
|--------|---------|-------------|
| `--iterations` | `3` | Measured runs per prompt/API combo |
| `--warmup` | `1` | Warmup runs (discarded) |
| `--max_tokens` | `512` | Max output tokens per request |
| `--temperature` | `0.7` | Sampling temperature |
| `--concurrency` | `5` | Parallel requests for throughput test |
| `--cache_rounds` | `5` | Identical calls for cache detection |
| `--apis` | `both` | `both`, `completions`, or `responses` |
| `--prompts` | `all` | `all` or comma-separated: `short,medium,long,code,reasoning` |
| `--timeout` | `120` | Request timeout (seconds) |
| `--output` | `all` | Export: `all`, `json`, `csv`, or `none` |

**Examples:**

```bash
# Test only Completions API with 5 iterations
python -m foundry_speedtest bench gpt-5.2 --apis completions --iterations 5

# Short + code prompts only, no file export
python -m foundry_speedtest bench gpt-4.1-nano --prompts short,code --output none

# High concurrency stress test
python -m foundry_speedtest bench gpt-4.1-nano --concurrency 20 --cache_rounds 10
```

### `quick` â€” Fast Connectivity Check

```bash
python -m foundry_speedtest quick <model>
```

Runs a single medium prompt across both APIs (streaming + sync) â€” good for verifying your setup works.

### `compare` â€” Model vs Model

```bash
python -m foundry_speedtest compare <model1> <model2>
```

Side-by-side comparison across short/medium/long prompts with a winner declared per metric.

### `list_prompts` â€” Show Prompt Catalogue

```bash
python -m foundry_speedtest list_prompts
```

---

## ğŸ“‚ Output & Reports

Results are saved to the `results/` directory:

| File | Contents |
|------|----------|
| `benchmark_<model>_<timestamp>.json` | Full structured results with nested stats |
| `benchmark_<model>_<timestamp>.csv` | Aggregate summary (one row per test) |
| `benchmark_<model>_<timestamp>_raw.csv` | Every individual API call logged |

---

## ğŸ§ª What Gets Tested

| Test | Streaming | Non-Streaming | Description |
|------|:---------:|:-------------:|-------------|
| **Short prompt** | âœ“ | âœ“ | Trivial question â†’ measures overhead |
| **Medium prompt** | âœ“ | âœ“ | Technical explanation â†’ balanced test |
| **Long prompt** | âœ“ | âœ“ | Tutorial generation â†’ heavy output |
| **Code generation** | âœ“ | âœ“ | Python function â†’ structured output |
| **Reasoning** | âœ“ | âœ“ | Multi-step logic â†’ reasoning latency |
| **Cache warm/cold** | âœ“ | â€” | Identical prompt repeated N times |
| **Concurrency** | â€” | âœ“ | Parallel requests for throughput |

---

## ğŸ” Authentication

This tool uses **Azure Entra ID** (via `DefaultAzureCredential`) â€” no API keys stored in code. It picks up credentials from:

1. `az login` session
2. Environment variables (`AZURE_CLIENT_ID`, etc.)
3. Managed Identity (when running in Azure)
4. VS Code Azure account

---

## License

MIT
